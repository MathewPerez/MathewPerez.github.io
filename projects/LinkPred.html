<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=XGMkxXUZTA64h2imyzu79g');ol{margin:0;padding:0}table td,table th{padding:0}.c16{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;background-color:#2b2b2b;border-left-style:solid;border-bottom-width:1pt;width:468pt;border-top-color:#000000;border-bottom-style:solid}.c15{background-color:#2b2b2b;color:#6897bb;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Courier New";font-style:normal}.c6{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left;height:11pt}.c2{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c14{background-color:#2b2b2b;color:#808080;font-weight:400;text-decoration:none;vertical-align:baseline;font-family:"Courier New";font-style:normal}.c8{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{background-color:#2b2b2b;font-size:10pt;font-family:"Courier New";color:#a9b7c6;font-weight:400}.c10{background-color:#2b2b2b;color:#9876aa;font-weight:400;font-family:"Courier New";font-style:italic}.c3{border-spacing:0;border-collapse:collapse;margin-right:auto}.c30{background-color:#2b2b2b;font-family:"Courier New";color:#4e807d;font-weight:400}.c5{background-color:#2b2b2b;font-family:"Courier New";color:#6a8759;font-weight:400}.c19{background-color:#2b2b2b;font-family:"Courier New";color:#6897bb;font-weight:400}.c13{background-color:#2b2b2b;font-family:"Courier New";color:#a9b7c6;font-weight:400}.c11{text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c22{background-color:#2b2b2b;font-family:"Courier New";color:#ffc66d;font-weight:400}.c12{background-color:#2b2b2b;font-family:"Courier New";color:#cc7832;font-weight:400}.c27{color:#000000;font-weight:400;font-size:12pt;font-family:"Arial"}.c26{color:#ffffff;font-weight:400;font-size:12pt;font-family:"Roboto Mono"}.c20{background-color:#2b2b2b;font-family:"Courier New";color:#808080;font-weight:400}.c23{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c21{color:#000000;font-weight:700;font-size:14pt;font-family:"Arial"}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c17{text-decoration:none;vertical-align:baseline;font-style:normal}.c28{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c18{color:inherit;text-decoration:inherit}.c29{font-size:12pt;font-weight:700}.c0{height:0pt}.c31{font-size:12pt}.c24{font-style:italic}.c9{font-size:10pt}.c25{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c28"><p class="c8"><span class="c17 c21">Spark Link Prediction</span></p><p class="c8"><span class="c23 c31"><a class="c18" href="https://www.google.com/url?q=https://github.com/MathewPerez/LinkPrediction&amp;sa=D&amp;ust=1611537887974000&amp;usg=AOvVaw0mS8ZyvbufB1yCpC6Txir6">Code Repository</a></span></p><p class="c1"><span class="c2"></span></p><p class="c8"><span class="c2">Intro</span></p><p class="c8"><span>We are trying to build a simple Scala command line program that will take in as input a user id and return a list of potential friends, across a social network. This can be accomplished by solving the graph problem of </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Link_prediction&amp;sa=D&amp;ust=1611537887975000&amp;usg=AOvVaw3ZeQFELdLkagrVrBQCgFsg">link prediction</a></span><span>. More specifically, we will use </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Link_prediction%23Jaccard_measure&amp;sa=D&amp;ust=1611537887976000&amp;usg=AOvVaw3ziPyjEzXH7yuyBSWB-fRf">Jaccard&rsquo;s measure</a></span><span>. Lastly, we can utilize Apache Spark&rsquo;s </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://spark.apache.org/docs/latest/graphx-programming-guide.html&amp;sa=D&amp;ust=1611537887976000&amp;usg=AOvVaw1AMaRuJKNkF3sbtOxyMtiM">GraphX</a></span><span>&nbsp;library to process graph data efficiently.</span></p><p class="c1"><span class="c2"></span></p><p class="c8"><span class="c2">Data Description</span></p><p class="c8"><span>Our dataset is the </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://snap.stanford.edu/data/ego-Facebook.html&amp;sa=D&amp;ust=1611537887977000&amp;usg=AOvVaw02VMTD3lfS1cVLHgVYqQVd">SNAP Facebook graph data</a></span><span class="c6">&nbsp;and we specifically use the facebook_combined.txt.gz dataset. It is a file where each line contains an edge from one vertex_id to another (i.e. basically an adjacency list). We note that each edge represents friends on Facebook; hence, the graph is undirected.</span></p><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c29">Environment Setup</span></p><p class="c8"><span>To write, debug, and refactor Spark Scala code, we will use </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://www.jetbrains.com/idea/download/&amp;sa=D&amp;ust=1611537887977000&amp;usg=AOvVaw3VX3V-U4RdlgH8VyrbcsKX">IntelliJ Idea Community Edition</a></span><span class="c6">. </span></p><p class="c8"><span class="c6">First, install the Scala plugin for IntelliJ, then create a Scala project of sbt type specifically. Next, ensure the SDK version is 1.8 (If not already installed, you can click Add SDK and select the 1.8 version to download). At the time of writing this, we use Scala version 2.12 (because of the Spark 3.0.1 version we&rsquo;ll choose later on) which may differ from the version of the latest Scala plugin for IntelliJ. However, using other versions of Scala besides 12 should work as long as the Scala version number and appropriate Spark version number are used in the build.sbt file. Create the IntelliJ project.</span></p><p class="c8"><span class="c6">Now, go to the build.sbt file and edit it to be like:</span></p><p class="c1"><span class="c6"></span></p><a id="t.c288527187748c01bd307421b9ab609d95ff2b5e"></a><a id="t.0"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c8"><span class="c13">name := </span><span class="c5 c11">&quot;LinkPrediction&quot;</span></p><p class="c1"><span class="c5 c11"></span></p><p class="c8"><span class="c13">version := </span><span class="c5 c11">&quot;0.1&quot;</span></p><p class="c1"><span class="c5 c11"></span></p><p class="c8"><span class="c13">scalaVersion := </span><span class="c5 c11">&quot;2.12.0&quot;</span></p><p class="c1"><span class="c5 c11"></span></p><p class="c8"><span class="c13 c11">libraryDependencies ++= Seq(</span></p><p class="c8"><span class="c13">&nbsp;</span><span class="c5">&quot;org.apache.spark&quot; </span><span class="c13">%% </span><span class="c5">&quot;spark-core&quot; </span><span class="c13">% </span><span class="c5">&quot;3.0.1&quot;</span><span class="c11 c12">,</span></p><p class="c8"><span class="c12">&nbsp;</span><span class="c5">&quot;org.apache.spark&quot; </span><span class="c13">%% </span><span class="c5">&quot;spark-graphx&quot; </span><span class="c13">% </span><span class="c5">&quot;3.0.1&quot;</span><span class="c11 c12">,</span></p><p class="c8"><span class="c13">)</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span>This way, the build of our project will find the Spark API via Maven. Also, go to the File tab of IntelliJ, then Project Structure, and then Project. Make sure that the Project SDK is 1.8 and that Project Language Level is set to &ldquo;8&rdquo; or something indicating Java8 is being used. We use SDK-1.8/Java8 here since it is the most stable release used with our version of Spark; however, this may change over time. Once again, note that in the build.sbt file, you could use different versions of Spark as long as the Scala version is adjusted accordingly. For more info, there is the </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://mvnrepository.com/artifact/org.apache.spark/spark-core&amp;sa=D&amp;ust=1611537887981000&amp;usg=AOvVaw1VgS7tmdvHdt6P0PN1owzP">Spark/Scala compatibilities</a></span><span>&nbsp;and the </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://repo1.maven.org/maven2/org/apache/spark/&amp;sa=D&amp;ust=1611537887982000&amp;usg=AOvVaw2sKcWVIQio3tjPoDNUjW8f">Maven Central Repository</a></span><span class="c6">&nbsp;to see what versions are currently up. Now, build the project in IntelliJ to make sure it succeeds. At this point, you could move the SNAP repository into the project for a simpler file path later on (you don&rsquo;t need to nest it in any folders or anything) or leave it wherever is easiest. </span></p><p class="c8"><span class="c6">Finally, we&rsquo;ll do a last sanity check to ensure that our Scala will compile properly and that the Spark dependencies will be located. Add a Scala Object under src/main/scala directory, called LinkPred.scala. Put the following code into it:</span></p><p class="c1"><span class="c6"></span></p><a id="t.4c519a6e8903c34c37f2c588d548174e77937aa8"></a><a id="t.1"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c8"><span class="c12 c9">import </span><span class="c7">org.apache.spark.{SparkConf</span><span class="c12 c9">, </span><span class="c7 c17">SparkContext}</span></p><p class="c8"><span class="c12 c9">import </span><span class="c7 c17">org.apache.spark.graphx.GraphLoader</span></p><p class="c1"><span class="c7 c17"></span></p><p class="c8"><span class="c12 c9">object </span><span class="c7 c17">LinkPred {</span></p><p class="c8"><span class="c7">&nbsp;</span><span class="c12 c9">def </span><span class="c7 c17">main(args: Array[String]): Unit = {</span></p><p class="c8"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Creates a Spark Context - Local Mode with two threads</span></p><p class="c8"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">conf = </span><span class="c12 c9">new </span><span class="c7">SparkConf().setAppName(</span><span class="c5 c9">&quot;LinkPred&quot;</span><span class="c7">).setMaster(</span><span class="c5 c9">&quot;local[2]&quot;</span><span class="c7 c17">)</span></p><p class="c8"><span class="c7">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">sc = </span><span class="c12 c9">new </span><span class="c7 c17">SparkContext(conf)</span></p><p class="c8"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Optional - set to decrease Spark&#39;s runtime output verbosity</span></p><p class="c8"><span class="c9 c20">&nbsp; &nbsp;</span><span class="c7">sc.setLogLevel(</span><span class="c5 c9">&quot;WARN&quot;</span><span class="c7 c17">)</span></p><p class="c8"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Import graph from edges file</span></p><p class="c8"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">graph = GraphLoader.edgeListFile(sc</span><span class="c12 c9">,</span><span class="c5 c9">&quot;/path/to/SNAP/directory/facebook_combined.txt&quot;</span><span class="c7 c17">)</span></p><p class="c8"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Count number of edges and vertices to ensure graphx api is functioning correct</span></p><p class="c8"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c7">println(</span><span class="c5 c9">&quot;Number of vertices : &quot; </span><span class="c7 c17">+ graph.vertices.count())</span></p><p class="c8"><span class="c7">&nbsp; &nbsp;println(</span><span class="c5 c9">&quot;Number of edges : &quot; </span><span class="c7 c17">+ graph.edges.count())</span></p><p class="c8"><span class="c7 c17">&nbsp;}</span></p><p class="c8"><span class="c7">}</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">This will print out the number of vertices and number of edges in the network. They should be approximately: </span></p><p class="c8"><span class="c6">Number of vertices : 4039</span></p><p class="c8"><span class="c6">Number of edges : 88234</span></p><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c29">Coding it up</span></p><p class="c8"><span>A basic knowledge of Apache Spark is assumed for this work (i.e. familiar with RDDs, partitioning of RDDs, how Spark can run on top of HDFS). It is recommended to read the entire </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://spark.apache.org/docs/latest/graphx-programming-guide.html&amp;sa=D&amp;ust=1611537887988000&amp;usg=AOvVaw2UBCjhV39gHTsSuQsIB6Hv">GraphX</a></span><span>&nbsp;overview to understand how we&rsquo;ll use its features for our use case. </span></p><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">We will first add to our current code by putting in the necessary imports:</span></p><p class="c1"><span class="c6"></span></p><a id="t.080cb21e679f7444a4fa4d97fb542d3a960c6f08"></a><a id="t.2"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c12 c9">import </span><span class="c7">org.apache.spark.{SparkConf</span><span class="c12 c9">, </span><span class="c7 c17">SparkContext}</span></p><p class="c4"><span class="c12 c9">import </span><span class="c7">org.apache.spark.graphx.{EdgeDirection</span><span class="c12 c9">, </span><span class="c7">GraphLoader</span><span class="c12 c9">, </span><span class="c7 c17">PartitionStrategy}</span></p><p class="c4"><span class="c12 c9">import </span><span class="c7">scala.collection.mutable.PriorityQueue</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">Then, at the top of our main function we&rsquo;ll add a simple command line input for the user we want to find potential friends for:</span></p><p class="c1"><span class="c6"></span></p><a id="t.069bf4ff1515d905fa480cd4161c2c3d460a3926"></a><a id="t.3"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c12 c9">val </span><span class="c7">userInput = scala.io.StdIn.readLine(</span><span class="c5 c9">&quot;Enter userid to find potential friends (Must be Int) : &quot;</span><span class="c7 c17">)</span></p><p class="c4"><span class="c12 c9">val </span><span class="c7">targetUserId = userInput.toInt</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">Next, we&rsquo;ll create our graph with a partitioning scheme:</span></p><p class="c1"><span class="c6"></span></p><a id="t.967fb206da982ef87e691427605a88364d1f8752"></a><a id="t.4"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c12 c9">val </span><span class="c7">graph = GraphLoader.</span><span class="c7 c24">edgeListFile</span><span class="c7">(sc</span><span class="c12 c9">, </span><span class="c5 c9">&quot;/Users/</span><span class="c5 c9">mathewperez</span><span class="c5 c9">/IdeaProjects/LinkPrediction/facebook/facebook_combined.txt&quot;</span><span class="c7 c17">)</span></p><p class="c4"><span class="c7">&nbsp;.partitionBy(PartitionStrategy.CanonicalRandomVertexCut)</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span>The </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://spark.apache.org/docs/latest/api/scala/org/apache/spark/graphx/Graph.html%23partitionBy(PartitionStrategy):Graph%5BVD,ED%5D&amp;sa=D&amp;ust=1611537887994000&amp;usg=AOvVaw10YN6KgEOmthbkEwB-huLQ">partitionBy</a></span><span>&nbsp;method comes from Graph class and we give it the CanonicalRandomVertexCut partitioning strategy as an argument. This partitioning scheme ensures identical undirected edges will be colocated. Obviously, this is not a huge factor when we are running our code locally and when our edges are not repeated, but it&rsquo;s an important method in many cases at scale. More </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://spark.apache.org/docs/latest/api/scala/org/apache/spark/graphx/PartitionStrategy.html&amp;sa=D&amp;ust=1611537887995000&amp;usg=AOvVaw3sPv4Fl0J4FNkk3sakMEkH">partition strategies</a></span><span class="c6">&nbsp;exist for different use cases (i.e. collocate directed edges, collocate source vertices, collocate destination vertices). By default, Graphx does not repartition edges when a graph is built, so edges are left in their default partitions (e.g. their original HDFS blocks).</span></p><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">Next, we find the friends list for each user in the network via:</span></p><p class="c1"><span class="c6"></span></p><a id="t.77b2197b7bc53f4b8296886827e74abd127e926d"></a><a id="t.5"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c12 c9">val </span><span class="c7">neighbors = graph.collectNeighborIds(EdgeDirection.</span><span class="c9 c10">Either</span><span class="c7">)</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">Note that we pass EdgeDirection.Either as an argument to ensure neighbors are counted both ways (i.e. treat it as an undirected graph, since Graphx uses directed multigraphs by default). </span></p><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">Now, we continue by filtering out users from our potential friends list. Namely, we exclude users who the target user is already friends with and the target user themselves obviously:</span></p><p class="c1"><span class="c6"></span></p><a id="t.99c4c91a188eac40d94d72ad8b344e2b310e7cb3"></a><a id="t.6"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c14 c9">// Find users who the target user is already friends with</span></p><p class="c4"><span class="c12 c9">val </span><span class="c7">currFriends = neighbors.lookup(targetUserId)(</span><span class="c19 c9">0</span><span class="c7 c17">)</span></p><p class="c4"><span class="c14 c9">// Limit our search to users they are not friends with and exclude the target user themself</span></p><p class="c4"><span class="c12 c9">val </span><span class="c7">availableUsers = neighbors.filter(v =&gt; !currFriends.contains(v._1) &amp;&amp; v._1.toInt != targetUserId)</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">Now, we&rsquo;ll go back and cache the neighbors RDD to improve performance. With our small dataset and local spark instance, only a 0.3 sec improvement was observed. However, at scale, caching RDDs is good practice in Spark programming. After, we&rsquo;re done with the neighbors RDD we can use unpersist to force it from memory (rather than relying on Spark to evict its partitions in LRU order) :</span></p><p class="c1"><span class="c6"></span></p><a id="t.c64df1937efc4a9e409fed02d8688ba691345512"></a><a id="t.7"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c14 c9">// Collect the friends list of each user</span></p><p class="c4"><span class="c12 c9">val </span><span class="c7">neighbors = graph.collectNeighborIds(EdgeDirection.</span><span class="c10 c9">Either</span><span class="c7 c17">).cache()</span></p><p class="c4"><span class="c14 c9">// Find users who the target user is already friends with</span></p><p class="c4"><span class="c12 c9">val </span><span class="c7">currFriends = neighbors.lookup(targetUserId)(</span><span class="c19 c9">0</span><span class="c7 c17">)</span></p><p class="c4"><span class="c14 c9">// Limit our search to users they are not friends with and exclude the target user themself</span></p><p class="c4"><span class="c12 c9">val </span><span class="c7 c17">availableUsers = neighbors.filter(v =&gt; !currFriends.contains(v._1) &amp;&amp; v._1.toInt != targetUserId)</span></p><p class="c4"><span class="c7">neighbors.unpersist()</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span>The final logic of our code is calculating the Jaccard measure for each potential friend and ranking them by the Jaccard measure in descending order. A note on the Jaccard measure is that its calculation is often too expensive, so a probabilistic approximation of Jaccard measure - </span><span class="c23"><a class="c18" href="https://www.google.com/url?q=https://towardsdatascience.com/scalable-jaccard-similarity-using-minhash-and-spark-85d00a007c5e&amp;sa=D&amp;ust=1611537888002000&amp;usg=AOvVaw15gfoyV7Z80sIzbOBGSy17">MinHash</a></span><span class="c6">&nbsp;- is used in practice. However, Spark MinHash methods and calculations require more overhead and are not necessary for our small dataset. </span></p><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">To compute the Jaccard measure for each potential friend and rank them, we&rsquo;ll first create a simple Pair class consisting of two attributes: UserId and Jaccard Measure. Then, we initialize a Max Heap/Priority Queue :</span></p><p class="c1"><span class="c6"></span></p><a id="t.33a920ecc7d61e35af94bf4ed67dff9547404a91"></a><a id="t.8"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c12 c9">final case class </span><span class="c7">Pair(userid: </span><span class="c12 c9">Int, </span><span class="c7">jaccard: </span><span class="c12 c9">Float</span><span class="c7 c17">){}</span></p><p class="c4"><span class="c12 c9">var </span><span class="c7">queue = PriorityQueue.empty[Pair](</span><span class="c10 c9">Ordering</span><span class="c7">.</span><span class="c7 c24">by</span><span class="c7">((_: Pair).jaccard))</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">Next, we&rsquo;ll iterate over the list of potential friends and calculate the Jaccard measure, then create the respective Pair object, and add this Pair to the heap :</span></p><p class="c1"><span class="c6"></span></p><a id="t.c3eb479a65e108964558482b7f2098cc0cca87e5"></a><a id="t.9"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c12 c9">for </span><span class="c7 c17">(x &lt;- availableUsers.collect()) {</span></p><p class="c4"><span class="c7">&nbsp;</span><span class="c12 c9">val </span><span class="c7 c17">ovlp_len = currFriends.intersect(x._2).length</span></p><p class="c4"><span class="c7">&nbsp;</span><span class="c9 c12">val </span><span class="c7 c17">insctn_len = currFriends.union(x._2).length</span></p><p class="c4"><span class="c7">&nbsp;</span><span class="c12 c9">val </span><span class="c7">jaccard = ovlp_len.</span><span class="c7">toFloat</span><span class="c7 c17">/insctn_len.toFloat</span></p><p class="c4"><span class="c7">&nbsp;</span><span class="c12 c9">val </span><span class="c7">p = </span><span class="c7 c24">Pair</span><span class="c7">(x._1.toInt</span><span class="c12 c9">, </span><span class="c7 c17">jaccard)</span></p><p class="c4"><span class="c7 c17">&nbsp;queue += p</span></p><p class="c4"><span class="c7">}</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">Note, we can use collect() on availableUsers to keep it in memory because the RDD is small enough. To finish it all off, we dequeue the top 10 Pairs and print out the user id&rsquo;s :</span></p><p class="c1"><span class="c6"></span></p><a id="t.49766e03910d6170918948b476b3bde7bdcd19f0"></a><a id="t.10"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c7 c24">println</span><span class="c7">(</span><span class="c5 c9">&quot;Potential Friends:&quot;</span><span class="c7 c17">)</span></p><p class="c4"><span class="c12 c9">var </span><span class="c7">i = </span><span class="c15 c9">0</span></p><p class="c4"><span class="c12 c9">while </span><span class="c7">(i &lt; </span><span class="c19 c9">10</span><span class="c7 c17">) {</span></p><p class="c4"><span class="c7">&nbsp;</span><span class="c7 c24">println</span><span class="c7 c17">(queue.dequeue().userid)</span></p><p class="c4"><span class="c7">&nbsp;i += </span><span class="c15 c9">1</span></p><p class="c4"><span class="c7">}</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p><p class="c8"><span class="c6">The final code should look like this:</span></p><p class="c1"><span class="c6"></span></p><a id="t.0eca51a803bba35e2c75b6caaad2b5983b748672"></a><a id="t.11"></a><table class="c3"><tbody><tr class="c0"><td class="c16" colspan="1" rowspan="1"><p class="c4"><span class="c12 c9">import </span><span class="c7">org.apache.spark.{SparkConf</span><span class="c12 c9">, </span><span class="c7 c17">SparkContext}</span></p><p class="c4"><span class="c12 c9">import </span><span class="c7">org.apache.spark.graphx.{EdgeDirection</span><span class="c12 c9">, </span><span class="c7">GraphLoader</span><span class="c12 c9">, </span><span class="c7 c17">PartitionStrategy}</span></p><p class="c4"><span class="c12 c9">import </span><span class="c7 c17">scala.collection.mutable.PriorityQueue</span></p><p class="c4 c25"><span class="c7 c17"></span></p><p class="c4"><span class="c12 c9">object </span><span class="c7 c17">LinkPred {</span></p><p class="c4"><span class="c7">&nbsp;</span><span class="c12 c9">def </span><span class="c9 c22">main</span><span class="c7">(args: Array[</span><span class="c9 c30">String</span><span class="c7">]): </span><span class="c12 c9">Unit </span><span class="c7 c17">= {</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c9 c14">// Ask for target userid</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">userInput = scala.io.StdIn.readLine(</span><span class="c5 c9">&quot;Enter userid to find potential friends (Must be Int) : &quot;</span><span class="c7 c17">)</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7 c17">targetUserId = userInput.toInt</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Creates a Spark Context - Local Mode with two threads</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">conf = </span><span class="c12 c9">new </span><span class="c7">SparkConf().setAppName(</span><span class="c5 c9">&quot;LinkPred&quot;</span><span class="c7">).setMaster(</span><span class="c5 c9">&quot;local[2]&quot;</span><span class="c7 c17">)</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">sc = </span><span class="c12 c9">new </span><span class="c7 c17">SparkContext(conf)</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Optional - set to decrease Spark&#39;s runtime output verbosity</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c7">sc.setLogLevel(</span><span class="c5 c9">&quot;WARN&quot;</span><span class="c7 c17">)</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Create graph from edge file</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">graph = GraphLoader.</span><span class="c7 c24">edgeListFile</span><span class="c7">(sc</span><span class="c12 c9">, </span><span class="c5 c9">&quot;/Users/</span><span class="c5 c9">mathewperez</span><span class="c5 c9">/IdeaProjects/LinkPrediction/facebook/facebook_combined.txt&quot;</span><span class="c7 c17">)</span></p><p class="c4"><span class="c7 c17">&nbsp; &nbsp; &nbsp;.partitionBy(PartitionStrategy.CanonicalRandomVertexCut)</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Collect the friends list of each user</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">neighbors = graph.collectNeighborIds(EdgeDirection.</span><span class="c10 c9">Either</span><span class="c7 c17">).cache()</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Find users who the target user is already friends with</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">currFriends = neighbors.lookup(targetUserId)(</span><span class="c19 c9">0</span><span class="c7 c17">)</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Limit our search to users they are not friends with and exclude the target user themself</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7 c17">availableUsers = neighbors.filter(v =&gt; !currFriends.contains(v._1) &amp;&amp; v._1.toInt != targetUserId)</span></p><p class="c4"><span class="c7 c17">&nbsp; &nbsp;neighbors.unpersist()</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Calculate and rank by Jaccard measure for each potential friend</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c12 c9">final case class </span><span class="c7">Pair(userid: </span><span class="c12 c9">Int, </span><span class="c7">jaccard: </span><span class="c12 c9">Float</span><span class="c7 c17">){}</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c12 c9">var </span><span class="c7">queue = PriorityQueue.empty[Pair](</span><span class="c10 c9">Ordering</span><span class="c7">.</span><span class="c7 c24">by</span><span class="c7 c17">((_: Pair).jaccard))</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c12 c9">for </span><span class="c7 c17">(x &lt;- availableUsers.collect()) {</span></p><p class="c4"><span class="c7">&nbsp; &nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7 c17">ovlp_len = currFriends.intersect(x._2).length</span></p><p class="c4"><span class="c7">&nbsp; &nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7 c17">insctn_len = currFriends.union(x._2).length</span></p><p class="c4"><span class="c7">&nbsp; &nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">jaccard = ovlp_len.</span><span class="c7">toFloat</span><span class="c7 c17">/insctn_len.toFloat</span></p><p class="c4"><span class="c7">&nbsp; &nbsp; &nbsp;</span><span class="c12 c9">val </span><span class="c7">p = </span><span class="c7 c24">Pair</span><span class="c7">(x._1.toInt</span><span class="c12 c9">, </span><span class="c7 c17">jaccard)</span></p><p class="c4"><span class="c7 c17">&nbsp; &nbsp; &nbsp;queue += p</span></p><p class="c4"><span class="c7 c17">&nbsp; &nbsp;}</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c14 c9">// Display potential friends list in desc order of Jaccard</span></p><p class="c4"><span class="c20 c9">&nbsp; &nbsp;</span><span class="c7 c24">println</span><span class="c7">(</span><span class="c5 c9">&quot;Potential Friends:&quot;</span><span class="c7 c17">)</span></p><p class="c4"><span class="c7">&nbsp; &nbsp;</span><span class="c12 c9">var </span><span class="c7">i = </span><span class="c15 c9">0</span></p><p class="c4"><span class="c19 c9">&nbsp; &nbsp;</span><span class="c12 c9">while </span><span class="c7">(i &lt; </span><span class="c9 c19">10</span><span class="c7 c17">) {</span></p><p class="c4"><span class="c7">&nbsp; &nbsp; &nbsp;</span><span class="c7 c24">println</span><span class="c7 c17">(queue.dequeue().userid)</span></p><p class="c4"><span class="c7">&nbsp; &nbsp; &nbsp;i += </span><span class="c9 c15">1</span></p><p class="c4"><span class="c19 c9">&nbsp; &nbsp;</span><span class="c7 c17">}</span></p><p class="c4"><span class="c7 c17">&nbsp;}</span></p><p class="c4"><span class="c7">}</span></p></td></tr></tbody></table><p class="c1"><span class="c6"></span></p></body></html>